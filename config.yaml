# ---- llm ----
api_base: http://localhost:11434/v1       
api_key: null
http_port: 8848
model: llama3.2                  # Specify the LLM to use
temperature: null                # Set default temperature parameter
top_p: null                      # Set default top-p parameter, range (0, 1)

# ---- agent ----
agents:
  demo: src/config/agents/demo/config.yaml
  weather: src/config/agents/weather/config.yaml
  coder: src/config/agents/coder/config.yaml

# ---- rag ----
rags:
  demo: src/config/rags/demo/config.yaml
  coder: src/config/rags/coder/config.yaml

# ---- tool ----
tools:
  web: src/config/tools/web/config.yaml
  fso: src/config/tools/fso/config.yaml
